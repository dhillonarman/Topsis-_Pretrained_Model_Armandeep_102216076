# Topsis-_Pretrained_Model_Armandeep_102216076
Topsis to find best pretrained model for text generation
TOPSIS for Text Generation Models
This project implements the TOPSIS (Technique for Order of Preference by Similarity to Ideal Solution) method to evaluate and rank pretrained text generation models based on three metrics: Perplexity, Diversity, and Inference Time. The models evaluated include GPT-2, XLNet, and Bloom.

Requirements
Before running the code, make sure to install the following dependencies:

bash
Copy
Edit
pip install numpy pandas matplotlib torch transformers datasets
Overview
This repository provides a Python script to:

Load pretrained text generation models (GPT-2, XLNet, and Bloom) using the transformers library.
Generate text sequences based on a prompt (Once upon a time,).
Evaluate the models using the following metrics:
Perplexity: Measures how well the model predicts the next word.
Diversity: The ratio of unique tokens generated by the model, indicating the variability in the generated text.
Inference Time: The time taken to generate text sequences.
Apply the TOPSIS method to rank the models based on the computed metrics.
How it Works
Data Collection:

The wikitext-2-raw-v1 dataset is used to extract a set of texts for evaluation.
A prompt ("Once upon a time,") is used to generate text sequences.
Model Evaluation:

For each model (GPT-2, XLNet, and Bloom), 5 text sequences are generated.
Perplexity, diversity, and inference time are calculated for each sequence.
TOPSIS Calculation:

The three metrics are normalized and weighted.
The ideal best and worst solutions are calculated for each metric.
A TOPSIS score is computed to rank the models.
Results:

The results are saved to a CSV file (topsis_results.csv).
A bar chart is generated to visually display the TOPSIS ranking of the models.
Evaluation Metrics
Perplexity: Measures how surprised the model is when generating text. Lower perplexity indicates better performance.

Diversity: The ratio of unique tokens in the generated texts. Higher diversity indicates that the model generates more varied content.

Inference Time: The average time taken for the model to generate a sequence of text.

TOPSIS Methodology
The TOPSIS method is used to evaluate multiple models based on the distance to an ideal solution. The steps include:

Normalization of the metrics.
Weighting of each metric (perplexity: -0.3, diversity: +0.4, inference time: -0.3).
Calculating the distance to the ideal best and worst solutions.
Computing the TOPSIS score for each model.
The model with the highest TOPSIS score is considered the best.

Results
The final ranking of the models based on the TOPSIS score is saved in topsis_results.csv. Additionally, a bar chart is plotted for visual analysis of the rankings.

Running the Code
To run the code, simply execute the following Python script:

bash
Copy
Edit
python topsis_text_generation.py
This will generate the evaluation results and save them to topsis_results.csv.

Example Output
The output will look like this:

css
Copy
Edit
Model           Perplexity   Diversity   Inference Time (s)   TOPSIS Score   Rank
GPT-2           32.5         0.68        0.45                 0.742         2
XLNet           28.9         0.74        0.52                 0.895         1
Bloom           35.7         0.65        0.60                 0.632         3
License
This project is licensed under the MIT License - see the LICENSE file for details.
